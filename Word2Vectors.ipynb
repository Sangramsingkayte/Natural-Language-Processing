{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'wikipedia-pubmed-and-PMC-w2v.bin'  # http://bio.nlplab.org/#word-vectors (download the data) \n",
    "print('Indexing word vectors')\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE,binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text file 'DRUG-AE.rel' which provides relations between drugs and adverse effects.\n",
    "TEXT_FILE = 'DRUG-AE.rel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists for the input fields and corresponding labels\n",
    "input_data_ae = []\n",
    "op_labels_ae = []\n",
    "\n",
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(TEXT_FILE, 'r')\n",
    "\n",
    "for each_line in f.readlines():\n",
    "    sent_list = np.zeros([0,200])\n",
    "    labels = np.zeros([0,3])\n",
    "    tokens = each_line.split(\"|\")\n",
    "    sent = tokens[1]\n",
    "    if sent in sentences:\n",
    "        continue\n",
    "    sentences.append(sent)\n",
    "    begin_offset = int(tokens[3])\n",
    "    end_offset = int(tokens[4])\n",
    "    mid_offset = range(begin_offset+1, end_offset)\n",
    "    word_tokens = nltk.word_tokenize(sent)\n",
    "    offset = 0\n",
    "    for each_token in word_tokens:\n",
    "        offset = sent.find(each_token, offset)\n",
    "        offset1 = copy.deepcopy(offset)\n",
    "        offset += len(each_token)\n",
    "        if each_token in punctuation or re.search(r'\\d', each_token):\n",
    "            continue\n",
    "        each_token = each_token.lower()\n",
    "        each_token = re.sub(\"[^A-Za-z\\-]+\",\"\", each_token)\n",
    "        if each_token in word2vec.vocab:\n",
    "            new_word = word2vec.word_vec(each_token)\n",
    "        if offset1 == begin_offset:\n",
    "            sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "            labels = np.append(labels, np.array([[0,0,1]]), axis=0)\n",
    "        elif offset == end_offset or offset in mid_offset:\n",
    "            sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "            labels = np.append(labels, np.array([[0,1,0]]), axis=0)\n",
    "        else:\n",
    "            sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "            labels = np.append(labels, np.array([[1,0,0]]), axis=0)\n",
    "\n",
    "    input_data_ae.append(sent_list)\n",
    "    op_labels_ae.append(labels)\n",
    "input_data_ae = np.array(input_data_ae)\n",
    "op_labels_ae  = np.array(op_labels_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_ae = pad_sequences(input_data_ae, maxlen=30, dtype='float64', padding='post')\n",
    "op_labels_ae = pad_sequences(op_labels_ae, maxlen=30, dtype='float64', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_data_ae))\n",
    "print(len(op_labels_ae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train and Validation datasets, for 4271 entries, 4000 in train dataset, and 271 in validation dataset\n",
    "x_train= input_data_ae[:4000]\n",
    "x_test = input_data_ae[4000:]\n",
    "y_train = op_labels_ae[:4000]\n",
    "y_test =op_labels_ae[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1      # Making the batch size as 1, as showing model each of the instances one-by-one\n",
    "# Adding Bidirectional LSTM with Dropout, and Time Distributed layer with Dropout\n",
    "# Finally using Adam optimizer for training purpose\n",
    "xin = Input(batch_shape=(batch,30,200), dtype='float')\n",
    "seq = Bidirectional(LSTM(300, return_sequences=True),merge_mode='concat')(xin)\n",
    "mlp1 = Dropout(0.2)(seq)\n",
    "mlp2 = TimeDistributed(Dense(60, activation='softmax'))(mlp1)\n",
    "mlp3 = Dropout(0.2)(mlp2)\n",
    "mlp4 = TimeDistributed(Dense(3, activation='softmax'))(mlp3)\n",
    "model = Model(inputs=xin, outputs=mlp4)\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch,\n",
    "          epochs=50,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(x_test,batch_size=batch)\n",
    "labels = []\n",
    "for i in range(len(val_pred)):\n",
    "    b = np.zeros_like(val_pred[i])\n",
    "    b[np.arange(len(val_pred[i])), val_pred[i].argmax(1)] = 1\n",
    "    labels.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score =[]\n",
    "f1 = []\n",
    "precision =[]\n",
    "recall =[]\n",
    "point = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_test)):\n",
    "    if(f1_score(labels[i],y_test[i],average='weighted')>.6):\n",
    "        point.append(i)\n",
    "    score.append(f1_score(labels[i],y_test[i],average='weighted'))\n",
    "    precision.append(precision_score(labels[i],y_test[i],average='weighted'))\n",
    "    recall.append(recall_score(labels[i],y_test[i],average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(point)/len(labels)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)\n",
    "print(\"\\n------x------\\n\")\n",
    "print(precision)\n",
    "print(\"\\n------x------\\n\")\n",
    "print(recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
