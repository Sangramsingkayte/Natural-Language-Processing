{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Importing the required packages\"\"\"\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import re \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make sure the dataset link is copied correctly\"\"\"\n",
    "\n",
    "dataset_link = 'http://mattmahoney.net/dc/'\n",
    "zip_file = 'text8.zip'\n",
    "\n",
    "def data_download(zip_file):\n",
    "    \"\"\"Downloading the required file\"\"\"\n",
    "    if not os.path.exists(zip_file):\n",
    "        zip_file, _ = urlretrieve(dataset_link + zip_file, zip_file)\n",
    "        print('File downloaded successfully!')\n",
    "    return None\n",
    "\n",
    "data_download(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extracting the dataset in separate folder\"\"\"\n",
    "extracted_folder = 'dataset'\n",
    "\n",
    "if not os.path.isdir(extracted_folder):\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        zf.extractall(extracted_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/text8') as ft_ :\n",
    "    full_text = ft_.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(ft8_text):\n",
    "    \"\"\"Replacing punctuation marks with tokens\"\"\"\n",
    "    ft8_text = ft8_text.lower()\n",
    "    ft8_text = ft8_text.replace('.', ' <period> ')\n",
    "    ft8_text = ft8_text.replace(',', ' <comma> ')\n",
    "    ft8_text = ft8_text.replace('\"', ' <quotation> ')\n",
    "    ft8_text = ft8_text.replace(';', ' <semicolon> ')\n",
    "    ft8_text = ft8_text.replace('!', ' <exclamation> ')\n",
    "    ft8_text = ft8_text.replace('?', ' <question> ')\n",
    "    ft8_text = ft8_text.replace('(', ' <paren_l> ')\n",
    "    ft8_text = ft8_text.replace(')', ' <paren_r> ')\n",
    "    ft8_text = ft8_text.replace('--', ' <hyphen> ')\n",
    "    ft8_text = ft8_text.replace(':', ' <colon> ')\n",
    "    ft8_text_tokens = ft8_text.split()\n",
    "    \n",
    "    return ft8_text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_tokens = text_processing(full_text)\n",
    "\"\"\"Shortlisting words with frequency more than 7\"\"\"\n",
    "word_cnt = collections.Counter(ft_tokens)\n",
    "shortlisted_words = [w for w in ft_tokens if word_cnt[w] > 7 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shortlisted_words[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of shortlisted words : \",len(shortlisted_words))\n",
    "print(\"Unique number of shortlisted words : \",len(set(shortlisted_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_creation(shortlisted_words):\n",
    "    \"\"\"The function creates a dictionary of the words present in dataset along with their frequency order\"\"\"\n",
    "    counts = collections.Counter(shortlisted_words)\n",
    "    vocabulary = sorted(counts, key=counts.get, reverse=True)\n",
    "    rev_dictionary_ = {ii: word for ii, word in enumerate(vocabulary)}\n",
    "    dictionary_ = {word: ii for ii, word in rev_dictionary_.items()}\n",
    "    return dictionary_, rev_dictionary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_, rev_dictionary_ = dict_creation(shortlisted_words)\n",
    "words_cnt = [dictionary_[word] for word in shortlisted_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating the threshold and performing the subsampling\"\"\"\n",
    "thresh = 0.00005\n",
    "word_counts = collections.Counter(words_cnt)\n",
    "total_count = len(words_cnt)\n",
    "freqs = {word: count / total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(thresh/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in words_cnt if p_drop[word] < random.random()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipG_target_set_generation(batch_, batch_index, word_window): \n",
    "    \"\"\"The function combines the words of given word_window size next to the index, for the SkipGram model\"\"\"\n",
    "    random_num = np.random.randint(1, word_window+1)\n",
    "    words_start = batch_index - random_num if (batch_index - random_num) > 0 else 0\n",
    "    words_stop = batch_index + random_num\n",
    "    window_target = set(batch_[words_start:batch_index] + batch_[batch_index+1:words_stop+1])\n",
    "    return list(window_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipG_batch_creation(short_words, batch_length, word_window):\n",
    "    \"\"\"The function internally makes use of the skipG_target_set_generation() function and combines each of the label \n",
    "    words in the shortlisted_words with the words of word_window size around\"\"\"\n",
    "    batch_cnt = len(short_words)//batch_length\n",
    "    short_words = short_words[:batch_cnt*batch_length]  \n",
    "    \n",
    "    for word_index in range(0, len(short_words), batch_length):\n",
    "        input_words, label_words = [], []\n",
    "        word_batch = short_words[word_index:word_index+batch_length]\n",
    "        for index_ in range(len(word_batch)):\n",
    "            batch_input = word_batch[index_]\n",
    "            batch_label = skipG_target_set_generation(word_batch, index_, word_window)\n",
    "            # Appending the label and inputs to the initial list. Replicating input to the size of labels in the window \n",
    "            label_words.extend(batch_label)\n",
    "            input_words.extend([batch_input]*len(batch_label))\n",
    "        yield input_words, label_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_graph = tf.Graph()\n",
    "with tf_graph.as_default():\n",
    "    input_ = tf.placeholder(tf.int32, [None], name='input_')\n",
    "    label_ = tf.placeholder(tf.int32, [None, None], name='label_')\n",
    "\n",
    "with tf_graph.as_default():\n",
    "    word_embed = tf.Variable(tf.random_uniform((len(rev_dictionary_), 300), -1, 1))\n",
    "    embedding = tf.nn.embedding_lookup(word_embed, input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.train.AdamOptimizer uses Kingma and Ba's Adam algorithm (http://arxiv.org/pdf/1412.6980v8.pdf) to control the\n",
    "learning rate.\n",
    "For further reference, one can refer to the following paper as well by Bengio, http://arxiv.org/pdf/1206.5533.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code includes the following  :\n",
    " # Initializing weights and bias to be used in the softmax layer\n",
    " # Loss function calculation using the Negative Sampling\n",
    " # Usage of Adam Optimizer\n",
    " # Negative sampling on 100 words, to be included in the loss function\n",
    " # 300 is the word embedding vector size\n",
    "\"\"\"\n",
    "vocabulary_size = len(rev_dictionary_)\n",
    "\n",
    "with tf_graph.as_default():\n",
    "    sf_weights = tf.Variable(tf.truncated_normal((vocabulary_size, 300), stddev=0.1) )\n",
    "    sf_bias = tf.Variable(tf.zeros(vocabulary_size) )\n",
    "\n",
    "    loss_fn = tf.nn.sampled_softmax_loss(weights=sf_weights, biases=sf_bias, \n",
    "                                         labels=label_, inputs=embedding, \n",
    "                                         num_sampled=100, num_classes=vocabulary_size)\n",
    "    cost_fn = tf.reduce_mean(loss_fn)\n",
    "    optim = tf.train.AdamOptimizer().minimize(cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The below code performs the following operations :\n",
    " # Performing validation here by making use of a random selection of 16 words from the dictionary of desired size\n",
    " # Selecting 8 words randomly from range of 1000    \n",
    " # Using the cosine distance to calculate the similarity between the words \n",
    "\"\"\"\n",
    "with tf_graph.as_default():\n",
    "    validation_cnt = 16\n",
    "    validation_dict = 100\n",
    "    \n",
    "    validation_words = np.array(random.sample(range(validation_dict), validation_cnt//2))\n",
    "    validation_words = np.append(validation_words, random.sample(range(1000,1000+validation_dict), validation_cnt//2))\n",
    "    validation_data = tf.constant(validation_words, dtype=tf.int32)\n",
    "\n",
    "    normalization_embed = word_embed / (tf.sqrt(tf.reduce_sum(tf.square(word_embed), 1, keep_dims=True)))\n",
    "    validation_embed = tf.nn.embedding_lookup(normalization_embed, validation_data)\n",
    "    word_similarity = tf.matmul(validation_embed, tf.transpose(normalization_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating the model checkpoint directory\"\"\"\n",
    "!mkdir model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2            # Increase it as per computation resources. It has been kept low here for users to replicate the process, increase to 100 or more\n",
    "batch_length = 1000\n",
    "word_window = 10\n",
    "\n",
    "with tf_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=tf_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = skipG_batch_creation(train_words, batch_length, word_window)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            train_loss, _ = sess.run([cost_fn, optim], \n",
    "                                     feed_dict={input_: x, label_: np.array(y)[:, None]})\n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs), \", Iteration: {}\".format(iteration),\n",
    "                      \", Avg. Training loss: {:.4f}\".format(loss/100),\", Processing : {:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 2000 == 0:\n",
    "                similarity_ = word_similarity.eval()\n",
    "                for i in range(validation_cnt):\n",
    "                    validated_words = rev_dictionary_[validation_words[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-similarity_[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % validated_words\n",
    "                    for k in range(top_k):\n",
    "                        close_word = rev_dictionary_[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"model_checkpoint/skipGram_text8.ckpt\")\n",
    "    embed_mat = sess.run(normalization_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The Saver class adds ops to save and restore variables to and from checkpoints.\"\"\"\n",
    "with tf_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=tf_graph) as sess:\n",
    "    \"\"\"Restoring the trained network\"\"\"\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('model_checkpoint'))\n",
    "    embed_mat = sess.run(word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the vector representation of 250 words to show their distribution across the new vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_graph = 250\n",
    "tsne = TSNE()\n",
    "word_embedding_tsne = tsne.fit_transform(embed_mat[:word_graph, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "for idx in range(word_graph):\n",
    "    plt.scatter(*word_embedding_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(rev_dictionary_[idx], (word_embedding_tsne[idx, 0], word_embedding_tsne[idx, 1]), alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('model_checkpoint', \"metainfo.tsv\"), 'w') as f:\n",
    "    for i in range(len(rev_dictionary_)):\n",
    "        f.write(rev_dictionary_[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter('model_checkpoint', sess.graph)\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding_conf = config.embeddings.add()\n",
    "embedding_conf.metadata_path = os.path.join('model_checkpoint', 'metainfo.tsv')\n",
    "projector.visualize_embeddings(summary_writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
