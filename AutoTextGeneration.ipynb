{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the vocabulary.txt file and the encoded datasets for Question and Answer are present in the same folder\n",
    "# reading vocabulary\n",
    "vocab_lines = open('vocabulary.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "# reading questions\n",
    "question_lines = open('InsuranceQAquestionanslabelraw.encoded', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "# reading answers\n",
    "answer_lines = open('InsuranceQAlabel2answerraw.encoded', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The print command shows the token value associated with each of the words in the 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" -- Vocabulary -- \")\n",
    "print(vocab_lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" -- Questions -- \")\n",
    "print(question_lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" -- Answers -- \")\n",
    "print(answer_lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "for line in vocab_lines:\n",
    "    _line = line.split('\\t')\n",
    "    if len(_line) == 2:\n",
    "        id2line[_line[0]] = _line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the word tokens for both questions and answers, along with the mapping of the answers enlisted for questions\n",
    "convs, ansid = [] , []\n",
    "for line in question_lines[:-1]:\n",
    "    _line = line.split('\\t')\n",
    "    ansid.append(_line[2].split(' '))\n",
    "    convs.append(_line[1])\n",
    "    \n",
    "convs1 = [ ]\n",
    "for line in answer_lines[:-1]:\n",
    "    _line = line.split('\\t')\n",
    "    convs1.append(_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convs[:2])  # word tokens present in the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ansid[:2])  # answers IDs mapped to the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convs1[:2])  # word tokens present in the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating matching pair between questions and answers on the basis of the ID allocated to each.\n",
    "\n",
    "questions, answers = [], []\n",
    "for a in range(len(ansid)):\n",
    "      for b in range(len(ansid[a])):\n",
    "            questions.append(convs[a])\n",
    "\n",
    "for a in range(len(ansid)):\n",
    "      for b in range(len(ansid[a])):\n",
    "            answers.append(convs1[int(ansid[a][b])-1])\n",
    "\n",
    "ques, ans =[], []\n",
    "\n",
    "m=0\n",
    "while m<len(questions):\n",
    "       i=0\n",
    "       a=[]\n",
    "       while i < (len(questions[m].split(' '))):\n",
    "            a.append(id2line[questions[m].split(' ')[i]])\n",
    "            i=i+1\n",
    "       ques.append(' '.join(a))\n",
    "       m=m+1\n",
    "\n",
    "n=0\n",
    "while n<len(answers):  \n",
    "        j=0\n",
    "        b=[]\n",
    "        while j < (len(answers[n].split(' '))):\n",
    "            b.append(id2line[answers[n].split(' ')[j]])\n",
    "            j=j+1\n",
    "        ans.append(' '.join(b))\n",
    "        n=n+1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing top 5 questions along with their answers\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(ques[i])\n",
    "    print(ans[i])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the count of the total number of questions and answers\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\t\"\"\"Cleaning the text by replacing the abbreviated words with their proper full replacement\"\"\"\n",
    "    \n",
    "\ttext = text.lower()\n",
    "\n",
    "\ttext = re.sub(r\"i'm\", \"i am\", text)\n",
    "\ttext = re.sub(r\"he's\", \"he is\", text)\n",
    "\ttext = re.sub(r\"she's\", \"she is\", text)\n",
    "\ttext = re.sub(r\"it's\", \"it is\", text)\n",
    "\ttext = re.sub(r\"that's\", \"that is\", text)\n",
    "\ttext = re.sub(r\"what's\", \"that is\", text)\n",
    "\ttext = re.sub(r\"where's\", \"where is\", text)\n",
    "\ttext = re.sub(r\"how's\", \"how is\", text)\n",
    "\ttext = re.sub(r\"\\'ll\", \" will\", text)\n",
    "\ttext = re.sub(r\"\\'ve\", \" have\", text)\n",
    "\ttext = re.sub(r\"\\'re\", \" are\", text)\n",
    "\ttext = re.sub(r\"\\'d\", \" would\", text)\n",
    "\ttext = re.sub(r\"\\'re\", \" are\", text)\n",
    "\ttext = re.sub(r\"won't\", \"will not\", text)\n",
    "\ttext = re.sub(r\"can't\", \"cannot\", text)\n",
    "\ttext = re.sub(r\"n't\", \" not\", text)\n",
    "\ttext = re.sub(r\"n'\", \"ng\", text)\n",
    "\ttext = re.sub(r\"'bout\", \"about\", text)\n",
    "\ttext = re.sub(r\"'til\", \"until\", text)\n",
    "\ttext = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,']\", \"\", text)\n",
    "\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the 'clean_text()' function on the set of Questions and Answers\n",
    "clean_questions = []\n",
    "for question in ques:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in ans:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(clean_questions[i])\n",
    "    print(clean_answers[i])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths.describe(percentiles=[0,0.25,0.5,0.75,0.85,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than 1 words and longer than 100 words.\n",
    "min_line_length, max_line_length = 2, 100\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp, short_answers_temp = [], []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions, short_answers = [], []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of questions:\", len(short_questions))\n",
    "print(\"# of answers:\", len(short_answers))\n",
    "print(\"% of data used: {}%\".format(round(len(short_questions)/len(questions),4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "\"\"\"Including <PAD> token in sentence to make all batches of same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "threshold = 1\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to provide a unique integer for each word.\n",
    "questions_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding unique tokens to the present vocabulary\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary so as to map the integers to their respective words, inverse of vocab_to_int\n",
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(questions_vocab_to_int))\n",
    "print(len(questions_int_to_vocab))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(answers_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to integers, and replacing any of the words not present in the respective vocabulary with <UNK> token \n",
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what percentage of all words have been replaced with <UNK>\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for question in questions_int:\n",
    "    for word in question:\n",
    "        if word == questions_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "for answer in answers_int:\n",
    "    for word in answer:\n",
    "        if word == answers_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "unk_ratio = round(unk_count/word_count,4)*100\n",
    "    \n",
    "print(\"Total number of words:\", word_count)\n",
    "print(\"Number of times <UNK> is used:\", unk_count)\n",
    "print(\"Percent of words that are <UNK>: {}%\".format(round(unk_ratio,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, sorting the questions and answers on basis of the length of the questions. \n",
    "# This exercise will reduce the amount of padding being done during the training process.\n",
    "# This will speed up the training process and reduce the training loss.\n",
    "\n",
    "sorted_questions = []\n",
    "short_questions1 = []\n",
    "sorted_answers = []\n",
    "short_answers1= []\n",
    "\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            short_questions1.append(short_questions[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])\n",
    "            short_answers1.append(short_answers[i[0]])\n",
    "            \n",
    "\n",
    "print(len(sorted_questions))\n",
    "print(len(sorted_answers))\n",
    "print(len(short_questions1))\n",
    "print(len(short_answers1))\n",
    "print()\n",
    "for i in range(3):\n",
    "    print(sorted_questions[i])\n",
    "    print(sorted_answers[i])\n",
    "    print(short_questions1[i])\n",
    "    print(short_answers1[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_questions[1547])\n",
    "print(short_questions1[1547])\n",
    "print(sorted_answers[1547])\n",
    "print(short_answers1[1547])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq helper functions for Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell, cell_bw = enc_cell,\n",
    "                                                   sequence_length = sequence_length, inputs = rnn_inputs, dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option=\"bahdanau\", num_units=dec_cell.output_size)\n",
    "    \n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], att_keys, att_vals,  att_score_fn, att_construct_fn,  name = \"attn_dec_train\")\n",
    "    \n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, train_decoder_fn,  dec_embed_input, sequence_length, scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    \n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option=\"bahdanau\", num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, encoder_state[0],  att_keys, att_vals,  att_score_fn, att_construct_fn, \n",
    "                        dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, name = \"attn_dec_inf\")\n",
    "    \n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, infer_decoder_fn, scope=decoding_scope)\n",
    "    \n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        \n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, None,  scope=decoding_scope, weights_initializer = weights, biases_initializer = biases)\n",
    "\n",
    "        train_logits = decoding_layer_train(encoder_state, dec_cell,  dec_embed_input, sequence_length,  decoding_scope, output_fn, keep_prob, batch_size)\n",
    "\n",
    "        decoding_scope.reuse_variables()\n",
    "        infer_logits = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'], \n",
    "                    sequence_length - 1, vocab_size,  decoding_scope, output_fn, keep_prob, batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size, \n",
    "                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, \n",
    "                  questions_vocab_to_int):\n",
    "    \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, answers_vocab_size+1,  enc_embedding_size, initializer = tf.random_uniform_initializer(0,1))\n",
    "    \n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "\n",
    "    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, dec_embeddings, enc_state, questions_vocab_size, \n",
    "                            sequence_length, rnn_size, num_layers, questions_vocab_to_int,  keep_prob, batch_size)\n",
    "    \n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model parameters\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Starting the session\n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "# Loading the model inputs    \n",
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "\n",
    "# Sequence length is max_line_length for each batch\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name='sequence_length')\n",
    "\n",
    "# Finding shape of the input data for sequence_loss\n",
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, inference_logits = seq2seq_model( tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int), \n",
    "    len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers,  questions_vocab_to_int)\n",
    "\n",
    "# Create inference logits tensor\n",
    "tf.identity(inference_logits, 'logits')\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Calculating Loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss( train_logits, targets, tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "    # Using Adam Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Performing Gradient Clipping to handle the vanishing gradient problem\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train and validation datasets for both questions and answers, with 15% to validation\n",
    "train_valid_split = int(len(sorted_questions)*0.15)\n",
    "\n",
    "train_questions = sorted_questions[train_valid_split:]\n",
    "train_answers = sorted_answers[train_valid_split:]\n",
    "\n",
    "valid_questions = sorted_questions[:train_valid_split]\n",
    "valid_answers = sorted_answers[:train_valid_split]\n",
    "\n",
    "print(len(train_questions))\n",
    "print(len(valid_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 20        # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 5                 # If the validation loss decreases after 5 consecutive checks, stop training\n",
    "validation_check = ((len(train_questions))//batch_size//2)-1        # Counter for checking validation loss\n",
    "total_train_loss = 0     # Record the training loss for each display step\n",
    "summary_valid_loss = []     # Record the validation loss for saving improvements in the model\n",
    "\n",
    "checkpoint= \"./best_model.ckpt\"   # creating the checkpoint file in the current directory\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_i in range(1, epochs+1):\n",
    "    for batch_i, (questions_batch, answers_batch) in enumerate(\n",
    "            batch_data(train_questions, train_answers, batch_size)):\n",
    "        start_time = time.time()\n",
    "        _, loss = sess.run(\n",
    "            [train_op, cost],\n",
    "            {input_data: questions_batch, targets: answers_batch,  lr: learning_rate, \n",
    "             sequence_length: answers_batch.shape[1], keep_prob: keep_probability})\n",
    "\n",
    "        total_train_loss += loss\n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "\n",
    "        if batch_i % display_step == 0:\n",
    "            print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                  .format(epoch_i, epochs, batch_i, \n",
    "                          len(train_questions) // batch_size, total_train_loss / display_step, \n",
    "                          batch_time*display_step))\n",
    "            total_train_loss = 0\n",
    "\n",
    "        if batch_i % validation_check == 0 and batch_i > 0:\n",
    "            total_valid_loss = 0\n",
    "            start_time = time.time()\n",
    "            for batch_ii, (questions_batch, answers_batch) in enumerate(batch_data(valid_questions, valid_answers, batch_size)):\n",
    "                valid_loss = sess.run(\n",
    "                cost, {input_data: questions_batch, targets: answers_batch, lr: learning_rate, \n",
    "                       sequence_length: answers_batch.shape[1], keep_prob: 1})\n",
    "                total_valid_loss += valid_loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "            avg_valid_loss = total_valid_loss / (len(valid_questions) / batch_size)\n",
    "            print('Valid Loss: {:>6.3f}, Seconds: {:>5.2f}'.format(avg_valid_loss, batch_time))\n",
    "            \n",
    "            # Reduce learning rate with a minimum value threshold\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "\n",
    "            summary_valid_loss.append(avg_valid_loss)\n",
    "            if avg_valid_loss <= min(summary_valid_loss):\n",
    "                print('New Record!') \n",
    "                stop_early = 0\n",
    "                saver = tf.train.Saver() \n",
    "                saver.save(sess, checkpoint)\n",
    "\n",
    "            else:\n",
    "                print(\"No Improvement.\")\n",
    "                stop_early += 1\n",
    "                if stop_early == stop:\n",
    "                    break\n",
    "    \n",
    "    if stop_early == stop:\n",
    "        print(\"Stopping Training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int):\n",
    "    \"\"\"Creating the question to be taken as input by the model\"\"\"\n",
    "    question = clean_text(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a random question from the full lot\n",
    "random = np.random.choice(len(short_questions))\n",
    "input_question = short_questions[random]\n",
    "print(input_question)\n",
    "\n",
    "# Transforming the selected question in the desired format of IDs and Words\n",
    "input_question = question_to_seq(input_question, questions_vocab_to_int)\n",
    "\n",
    "# Applying Padding to the question to reach the max_line_length\n",
    "input_question = input_question + [questions_vocab_to_int[\"<PAD>\"]] * (max_line_length - len(input_question))\n",
    "\n",
    "# Correcting the shape of input_data, by adding the empty questions \n",
    "batch_shell = np.zeros((batch_size, max_line_length))\n",
    "\n",
    "# Setting the input question as the first question\n",
    "batch_shell[0] = input_question    \n",
    "    \n",
    "# Passing  input question to the model\n",
    "answer_logits = sess.run(inference_logits, {input_data: batch_shell, \n",
    "                                            keep_prob: 1.0})[0]\n",
    "\n",
    "# Removing padding from Question and Answer both\n",
    "pad_q = questions_vocab_to_int[\"<PAD>\"]\n",
    "pad_a = answers_vocab_to_int[\"<PAD>\"]\n",
    "\n",
    "# Printing the final Answer output by the model \n",
    "print('Question')\n",
    "print('  Word Ids:      {}'.format([i for i in input_question if i != pad_q]))\n",
    "print('  Input Words: {}'.format([questions_int_to_vocab[i] for i in input_question if i != pad_q]))\n",
    "print('\\n')\n",
    "print(' '.join(([questions_int_to_vocab[i] for i in input_question if i != pad_q])))\n",
    "\n",
    "print('\\nAnswer')\n",
    "print('  Word Ids:      {}'.format([i for i in np.argmax(answer_logits, 1) if i != pad_a]))\n",
    "print('  Response Words: {}'.format([answers_int_to_vocab[i] for i in np.argmax(answer_logits, 1) if i != pad_a]))\n",
    "print('\\n')\n",
    "print(' '.join(([answers_int_to_vocab[i] for i in np.argmax(answer_logits, 1) if i != pad_a])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
