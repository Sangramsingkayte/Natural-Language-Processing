{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Importing the required packages\"\"\"\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import re \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make sure the dataset link is copied correctly\"\"\"\n",
    "\n",
    "dataset_link = 'http://mattmahoney.net/dc/'\n",
    "zip_file = 'text8.zip'\n",
    "\n",
    "def data_download(zip_file):\n",
    "    \"\"\"Downloading the required file\"\"\"\n",
    "    if not os.path.exists(zip_file):\n",
    "        zip_file, _ = urlretrieve(dataset_link + zip_file, zip_file)\n",
    "        print('File downloaded successfully!')\n",
    "    return None\n",
    "\n",
    "data_download(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extracting the dataset in separate folder\"\"\"\n",
    "extracted_folder = 'dataset'\n",
    "\n",
    "if not os.path.isdir(extracted_folder):\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        zf.extractall(extracted_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/text8') as ft_ :\n",
    "    full_text = ft_.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(ft8_text):\n",
    "    \"\"\"Replacing punctuation marks with tokens\"\"\"\n",
    "    ft8_text = ft8_text.lower()\n",
    "    ft8_text = ft8_text.replace('.', ' <period> ')\n",
    "    ft8_text = ft8_text.replace(',', ' <comma> ')\n",
    "    ft8_text = ft8_text.replace('\"', ' <quotation> ')\n",
    "    ft8_text = ft8_text.replace(';', ' <semicolon> ')\n",
    "    ft8_text = ft8_text.replace('!', ' <exclamation> ')\n",
    "    ft8_text = ft8_text.replace('?', ' <question> ')\n",
    "    ft8_text = ft8_text.replace('(', ' <paren_l> ')\n",
    "    ft8_text = ft8_text.replace(')', ' <paren_r> ')\n",
    "    ft8_text = ft8_text.replace('--', ' <hyphen> ')\n",
    "    ft8_text = ft8_text.replace(':', ' <colon> ')\n",
    "    ft8_text_tokens = ft8_text.split()\n",
    "    \n",
    "    return ft8_text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_tokens = text_processing(full_text)\n",
    "\"\"\"Shortlisting words with frequency more than 7\"\"\"\n",
    "word_cnt = collections.Counter(ft_tokens)\n",
    "shortlisted_words = [w for w in ft_tokens if word_cnt[w] > 7 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_creation(shortlisted_words):\n",
    "    \"\"\"The function creates a dictionary of the words present in dataset along with their frequency order\"\"\"\n",
    "    counts = collections.Counter(shortlisted_words)\n",
    "    vocabulary = sorted(counts, key=counts.get, reverse=True)\n",
    "    rev_dictionary_ = {ii: word for ii, word in enumerate(vocabulary)}\n",
    "    dictionary_ = {word: ii for ii, word in rev_dictionary_.items()}\n",
    "    return dictionary_, rev_dictionary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_, rev_dictionary_ = dict_creation(shortlisted_words)\n",
    "words_cnt = [dictionary_[word] for word in shortlisted_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def cbow_batch_creation(batch_length, word_window):\n",
    "    \"\"\"The function creates a batch with the list of the label words and list of their corresponding words in the context of\n",
    "    the label word.\"\"\"\n",
    "    global data_index\n",
    "    \"\"\"Pulling out the centered label word, and its next word_window count of surrounding words\n",
    "    word_window : window of words on either side of the center word\n",
    "    relevant_words : length of the total words to be picked in a single batch, \n",
    "            including the center word and the word_window words on both sides\n",
    "    Format :  [ word_window ... target ... word_window ] \"\"\"\n",
    "    relevant_words = 2 * word_window + 1 \n",
    "\n",
    "    batch = np.ndarray(shape=(batch_length,relevant_words-1), dtype=np.int32)\n",
    "    label_ = np.ndarray(shape=(batch_length, 1), dtype=np.int32)\n",
    "\n",
    "    buffer = collections.deque(maxlen=relevant_words)   # Queue to add/pop\n",
    "\n",
    "    #Selecting the words of length 'relevant_words' from the starting index \n",
    "    for _ in range(relevant_words):\n",
    "        buffer.append(words_cnt[data_index])\n",
    "        data_index = (data_index + 1) % len(words_cnt)\n",
    "\n",
    "    for i in range(batch_length):\n",
    "        target = word_window  # Center word as the label\n",
    "        target_to_avoid = [ word_window ] # Excluding the label, and selecting only the surrounding words\n",
    "\n",
    "        # add selected target to avoid_list for next time\n",
    "        col_idx = 0\n",
    "        for j in range(relevant_words):\n",
    "            if j==relevant_words//2:\n",
    "                continue\n",
    "            batch[i,col_idx] = buffer[j] # Iterating till the middle element for window_size length\n",
    "            col_idx += 1\n",
    "        label_[i, 0] = buffer[target]\n",
    "\n",
    "        buffer.append(words_cnt[data_index])\n",
    "        data_index = (data_index + 1) % len(words_cnt)\n",
    "\n",
    "    assert batch.shape[0]==batch_length and batch.shape[1]== relevant_words-1\n",
    "    return batch, label_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with num_skips = 2 and word_window = 1:\n",
      "    batch: [['anarchism', 'as'], ['originated', 'a'], ['as', 'term'], ['a', 'of'], ['term', 'abuse'], ['of', 'first'], ['abuse', 'used'], ['first', 'against']]\n",
      "    label_: ['originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used']\n",
      "\n",
      "with num_skips = 4 and word_window = 2:\n",
      "    batch: [['anarchism', 'originated', 'a', 'term'], ['originated', 'as', 'term', 'of'], ['as', 'a', 'of', 'abuse'], ['a', 'term', 'abuse', 'first'], ['term', 'of', 'first', 'used'], ['of', 'abuse', 'used', 'against'], ['abuse', 'first', 'against', 'early'], ['first', 'used', 'early', 'working']]\n",
      "    label_: ['as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "for num_skips, word_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, label_ = cbow_batch_creation(batch_length=8, word_window=word_window)\n",
    "    print('\\nwith num_skips = %d and word_window = %d:' % (num_skips, word_window))\n",
    "    \n",
    "    print('    batch:', [[rev_dictionary_[bii] for bii in bi] for bi in batch])\n",
    "    print('    label_:', [rev_dictionary_[li] for li in label_.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100001\n",
    "\"\"\"Initialzing :\n",
    "   # 128 is the length of the batch considered for CBOW\n",
    "   # 128 is the word embedding vector size\n",
    "   # Considering 1 word on both sides of the center label words\n",
    "   # Consider the center label word 2 times to create the batches\n",
    "\"\"\"\n",
    "batch_length = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The below code performs the following operations :\n",
    " # Performing validation here by making use of a random selection of 16 words from the dictionary of desired size\n",
    " # Selecting 8 words randomly from range of 1000    \n",
    " # Using the cosine distance to calculate the similarity between the words \n",
    "\"\"\"\n",
    "\n",
    "tf_cbow_graph = tf.Graph()\n",
    "\n",
    "with tf_cbow_graph.as_default():\n",
    "    validation_cnt = 16\n",
    "    validation_dict = 100\n",
    "    \n",
    "    validation_words = np.array(random.sample(range(validation_dict), validation_cnt//2))\n",
    "    validation_words = np.append(validation_words,random.sample(range(1000,1000+validation_dict), validation_cnt//2))\n",
    "\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_length,2*skip_window])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_length, 1])\n",
    "    validation_data = tf.constant(validation_words, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Embeddings for all the words present in the vocabulary\n",
    "\"\"\"\n",
    "with tf_cbow_graph.as_default() :\n",
    "    vocabulary_size = len(rev_dictionary_)\n",
    "    \n",
    "    word_embed = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "    # Averaging embeddings accross the full context into a single embedding layer\n",
    "    context_embeddings = []\n",
    "    for i in range(2*skip_window):\n",
    "        context_embeddings.append(tf.nn.embedding_lookup(word_embed, train_dataset[:,i]))\n",
    "\n",
    "    embedding =  tf.reduce_mean(tf.stack(axis=0,values=context_embeddings),0,keep_dims=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below function optimizes the weights, biases and word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:1444: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /Users/sangram/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The code includes the following  :\n",
    " # Initializing weights and bias to be used in the softmax layer\n",
    " # Loss function calculation using the Negative Sampling\n",
    " # Usage of AdaGrad Optimizer\n",
    " # Negative sampling on 64 words, to be included in the loss function\n",
    "\"\"\"\n",
    "with tf_cbow_graph.as_default() :\n",
    "    sf_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                     stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    sf_bias = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    loss_fn = tf.nn.sampled_softmax_loss(weights=sf_weights, biases=sf_bias, inputs=embedding,\n",
    "                           labels=train_labels, num_sampled=64, num_classes=vocabulary_size)\n",
    "    cost_fn = tf.reduce_mean(loss_fn)\n",
    "    \"\"\"Using AdaGrad as optimizer\"\"\"\n",
    "    optim = tf.train.AdagradOptimizer(1.0).minimize(cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-73b85abda2a3>:6: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Using the cosine distance to calculate the similarity between the batches and embeddings of other words \n",
    "\"\"\"\n",
    "with tf_cbow_graph.as_default() :\n",
    "    \n",
    "    normalization_embed = word_embed / tf.sqrt(tf.reduce_sum(tf.square(word_embed), 1, keep_dims=True))\n",
    "    validation_embed = tf.nn.embedding_lookup(normalization_embed, validation_data)\n",
    "    word_similarity = tf.matmul(validation_embed, tf.transpose(normalization_embed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 8.017372\n",
      "Nearest to use: lipids, workshops, printed, throttle, catharism, legally, unchallenged, farms,\n",
      "Nearest to a: ile, kings, ender, autosomal, archaeological, grande, cellulose, vivre,\n",
      "Nearest to people: doi, corsair, slums, deltas, suffocation, prescribing, anthony, emmaus,\n",
      "Nearest to that: organized, highway, countable, bocks, judaea, tel, divisive, sunglasses,\n",
      "Nearest to b: delusional, spock, tube, carvalho, capybaras, vegeta, sequitur, stratigraphic,\n",
      "Nearest to an: kaolin, functionaries, zamenhof, ibn, jets, drowsiness, peerless, tenggara,\n",
      "Nearest to there: collided, subfields, ordained, kapoor, toynbee, developers, neberg, diabetic,\n",
      "Nearest to two: garrett, handled, flavoured, chrono, ride, pfeiffer, walls, groot,\n",
      "Nearest to egypt: artery, alaskan, jeroboam, mitral, separation, mersenne, gpmg, relaxed,\n",
      "Nearest to behind: szko, hibiscus, pontiff, mccovey, coalescence, strongest, elbow, hawkeye,\n",
      "Nearest to institute: morphine, ytterbium, traditions, dabbled, ambience, macron, nfc, mutilated,\n",
      "Nearest to engineering: jurisdiction, bix, grammar, gaiman, maglev, uplifting, paws, latterly,\n",
      "Nearest to construction: barred, cushitic, carbonation, seized, rijeka, bobble, ladd, lifeform,\n",
      "Nearest to assembly: strains, drought, shelf, olives, clips, declining, dreamt, expellees,\n",
      "Nearest to articles: marshalling, inexplicable, drones, diabelli, bursting, photo, telnet, calista,\n",
      "Nearest to road: faust, quarterfinals, roleplaying, measurements, thousands, demented, husk, fax,\n",
      "Average loss at step 2000: 4.072459\n",
      "Average loss at step 4000: 3.531590\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=tf_cbow_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    avg_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_words, batch_label_ = cbow_batch_creation(batch_length, skip_window)\n",
    "        _, l = sess.run([optim, loss_fn], feed_dict={train_dataset : batch_words, train_labels : batch_label_ })\n",
    "        avg_loss += l\n",
    "        if step % 2000 == 0 :\n",
    "            if step > 0 :\n",
    "                avg_loss = avg_loss / 2000\n",
    "            print('Average loss at step %d: %f' % (step, np.mean(avg_loss) )) \n",
    "            avg_loss = 0\n",
    "        \n",
    "        if step % 10000 == 0:\n",
    "            sim = word_similarity.eval()\n",
    "            for i in range(validation_cnt):\n",
    "                valid_word = rev_dictionary_[validation_words[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = rev_dictionary_[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalization_embed.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the vector representation of 250 words to show their distribution across the new vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 250\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "embeddings_2d = tsne.fit_transform(final_embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(12,12))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [rev_dictionary_[i] for i in range(1, num_points+1)]\n",
    "cbow_plot(embeddings_2d, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
